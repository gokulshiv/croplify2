{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GtKYreu7iwhx",
        "outputId": "cb896dc2-be55-4ec4-ba3c-5b955cc02533"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-4.2.1-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.14.1-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (24.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.38)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna)\n",
            "  Downloading Mako-1.3.9-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.11/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n",
            "Downloading optuna-4.2.1-py3-none-any.whl (383 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.6/383.6 kB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.14.1-py3-none-any.whl (233 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.6/233.6 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Downloading Mako-1.3.9-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: Mako, colorlog, alembic, optuna\n",
            "Successfully installed Mako-1.3.9 alembic-1.14.1 colorlog-6.9.0 optuna-4.2.1\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (2.1.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from xgboost) (1.26.4)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.21.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from xgboost) (1.13.1)\n",
            "Collecting imblearn\n",
            "  Downloading imblearn-0.0-py2.py3-none-any.whl.metadata (355 bytes)\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.11/dist-packages (from imblearn) (0.13.0)\n",
            "Requirement already satisfied: numpy<3,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn->imblearn) (1.26.4)\n",
            "Requirement already satisfied: scipy<2,>=1.10.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn->imblearn) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn<2,>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn->imblearn) (1.6.1)\n",
            "Requirement already satisfied: sklearn-compat<1,>=0.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn->imblearn) (0.1.3)\n",
            "Requirement already satisfied: joblib<2,>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn->imblearn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn->imblearn) (3.5.0)\n",
            "Downloading imblearn-0.0-py2.py3-none-any.whl (1.9 kB)\n",
            "Installing collected packages: imblearn\n",
            "Successfully installed imblearn-0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-25 23:58:51,778] A new study created in memory with name: no-name-b45f7aea-e9db-4fe3-9feb-860bdb3722a6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading and preprocessing data...\n",
            "Applying SMOTE for class balancing...\n",
            "Optimizing hyperparameters...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-25 23:59:29,022] Trial 0 finished with value: 0.6337074386866919 and parameters: {'n_estimators': 400, 'max_depth': 7, 'learning_rate': 0.0902966227631913, 'subsample': 0.7290081745672452, 'colsample_bytree': 0.8531157258605591, 'gamma': 0.10924260124889326, 'reg_lambda': 0.09587822027128405, 'reg_alpha': 0.01979499013941734}. Best is trial 0 with value: 0.6337074386866919.\n",
            "[I 2025-02-25 23:59:56,221] Trial 1 finished with value: 0.4513841322969954 and parameters: {'n_estimators': 250, 'max_depth': 3, 'learning_rate': 0.03671461852558892, 'subsample': 0.6706225994420606, 'colsample_bytree': 0.8351465221712647, 'gamma': 0.026839576163925974, 'reg_lambda': 0.39143615689137384, 'reg_alpha': 0.004273472603650687}. Best is trial 0 with value: 0.6337074386866919.\n",
            "[I 2025-02-26 00:01:21,937] Trial 2 finished with value: 0.5888458519163914 and parameters: {'n_estimators': 500, 'max_depth': 5, 'learning_rate': 0.0334654085048001, 'subsample': 0.6609694858565651, 'colsample_bytree': 0.8058966208607651, 'gamma': 0.028890366826901766, 'reg_lambda': 0.8651859936307754, 'reg_alpha': 0.8228120758215135}. Best is trial 0 with value: 0.6337074386866919.\n",
            "[I 2025-02-26 00:01:58,784] Trial 3 finished with value: 0.5809654851148627 and parameters: {'n_estimators': 450, 'max_depth': 8, 'learning_rate': 0.09596102030053011, 'subsample': 0.6357143780844186, 'colsample_bytree': 0.7508437839657917, 'gamma': 0.23059746762536215, 'reg_lambda': 0.16303660562033379, 'reg_alpha': 0.40589914104219654}. Best is trial 0 with value: 0.6337074386866919.\n",
            "[I 2025-02-26 00:02:28,206] Trial 4 finished with value: 0.649904676460693 and parameters: {'n_estimators': 150, 'max_depth': 8, 'learning_rate': 0.06339418029697576, 'subsample': 0.7692126025596401, 'colsample_bytree': 0.6075632366547004, 'gamma': 0.018695634172776927, 'reg_lambda': 0.132994411125641, 'reg_alpha': 0.002252416325560562}. Best is trial 4 with value: 0.649904676460693.\n",
            "[I 2025-02-26 00:03:02,002] Trial 5 finished with value: 0.5726546527376403 and parameters: {'n_estimators': 400, 'max_depth': 4, 'learning_rate': 0.06143397230574366, 'subsample': 0.8072978901909943, 'colsample_bytree': 0.6593273844587908, 'gamma': 0.27835207372466336, 'reg_lambda': 0.031505546750709934, 'reg_alpha': 0.0912075937219507}. Best is trial 4 with value: 0.649904676460693.\n",
            "[I 2025-02-26 00:03:33,619] Trial 6 finished with value: 0.5352809240776045 and parameters: {'n_estimators': 300, 'max_depth': 3, 'learning_rate': 0.056568833111474315, 'subsample': 0.7032801323834574, 'colsample_bytree': 0.8844132258814885, 'gamma': 0.2711276291366024, 'reg_lambda': 0.5383235495801623, 'reg_alpha': 0.006306558691457736}. Best is trial 4 with value: 0.649904676460693.\n",
            "[I 2025-02-26 00:04:03,793] Trial 7 finished with value: 0.6129432975906004 and parameters: {'n_estimators': 250, 'max_depth': 4, 'learning_rate': 0.08349482151413905, 'subsample': 0.6660637532378625, 'colsample_bytree': 0.7240069974919858, 'gamma': 0.02067197434137297, 'reg_lambda': 0.0010685340298471745, 'reg_alpha': 0.04835059167564719}. Best is trial 4 with value: 0.649904676460693.\n",
            "[I 2025-02-26 00:04:43,986] Trial 8 finished with value: 0.5946653324246685 and parameters: {'n_estimators': 400, 'max_depth': 3, 'learning_rate': 0.07262456196972158, 'subsample': 0.8718517683838625, 'colsample_bytree': 0.8889788414173249, 'gamma': 0.015770757054479348, 'reg_lambda': 0.1045147853150599, 'reg_alpha': 0.0059615023187616625}. Best is trial 4 with value: 0.649904676460693.\n",
            "[I 2025-02-26 00:05:08,802] Trial 9 finished with value: 0.4937431526643145 and parameters: {'n_estimators': 200, 'max_depth': 4, 'learning_rate': 0.03199101058364886, 'subsample': 0.6117205389247009, 'colsample_bytree': 0.6048076011370572, 'gamma': 0.10858140044651825, 'reg_lambda': 0.07134183064536545, 'reg_alpha': 0.3613047056322168}. Best is trial 4 with value: 0.649904676460693.\n",
            "[I 2025-02-26 00:05:34,846] Trial 10 finished with value: 0.5763899552281295 and parameters: {'n_estimators': 100, 'max_depth': 10, 'learning_rate': 0.018365947608052453, 'subsample': 0.7870963569333334, 'colsample_bytree': 0.6023131883180606, 'gamma': 0.047337396872269705, 'reg_lambda': 0.013896141117485182, 'reg_alpha': 0.001090783333318137}. Best is trial 4 with value: 0.649904676460693.\n",
            "[I 2025-02-26 00:05:58,172] Trial 11 finished with value: 0.4779703418707569 and parameters: {'n_estimators': 100, 'max_depth': 7, 'learning_rate': 0.011709507952473825, 'subsample': 0.7363865183082365, 'colsample_bytree': 0.7094941709685658, 'gamma': 0.10068387679646802, 'reg_lambda': 0.011972159611446114, 'reg_alpha': 0.015503244134799607}. Best is trial 4 with value: 0.649904676460693.\n",
            "[I 2025-02-26 00:06:49,660] Trial 12 finished with value: 0.6548847922291905 and parameters: {'n_estimators': 350, 'max_depth': 9, 'learning_rate': 0.0506983354079385, 'subsample': 0.7771308838826951, 'colsample_bytree': 0.7887395124008596, 'gamma': 0.059019097754483524, 'reg_lambda': 0.04136336544199296, 'reg_alpha': 0.00186300389836678}. Best is trial 12 with value: 0.6548847922291905.\n",
            "[I 2025-02-26 00:07:27,670] Trial 13 finished with value: 0.6552980046755981 and parameters: {'n_estimators': 150, 'max_depth': 9, 'learning_rate': 0.04831210777153343, 'subsample': 0.8071261655565851, 'colsample_bytree': 0.7627736877058678, 'gamma': 0.01001229152066639, 'reg_lambda': 0.005111138889635692, 'reg_alpha': 0.0010535955675749916}. Best is trial 13 with value: 0.6552980046755981.\n",
            "[I 2025-02-26 00:08:23,002] Trial 14 finished with value: 0.6652677254337005 and parameters: {'n_estimators': 300, 'max_depth': 10, 'learning_rate': 0.04477880981416993, 'subsample': 0.8515827698858095, 'colsample_bytree': 0.7820311199831482, 'gamma': 0.011427377339104731, 'reg_lambda': 0.0030781068111066807, 'reg_alpha': 0.001049083225551397}. Best is trial 14 with value: 0.6652677254337005.\n",
            "[I 2025-02-26 00:09:10,432] Trial 15 finished with value: 0.6636036611140346 and parameters: {'n_estimators': 200, 'max_depth': 10, 'learning_rate': 0.04248987797998723, 'subsample': 0.8466700236715947, 'colsample_bytree': 0.7649042494407862, 'gamma': 0.010608701348139123, 'reg_lambda': 0.0016031018397890393, 'reg_alpha': 0.0024416068771144503}. Best is trial 14 with value: 0.6652677254337005.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters: {'n_estimators': 300, 'max_depth': 10, 'learning_rate': 0.04477880981416993, 'subsample': 0.8515827698858095, 'colsample_bytree': 0.7820311199831482, 'gamma': 0.011427377339104731, 'reg_lambda': 0.0030781068111066807, 'reg_alpha': 0.001049083225551397}\n",
            "Best CV accuracy: 0.6653\n",
            "Training final model...\n",
            "\n",
            "Final Model Accuracy: 0.7591\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.86      0.86        14\n",
            "           1       0.67      0.86      0.75        14\n",
            "           2       0.73      0.79      0.76        14\n",
            "           3       0.76      0.93      0.84        14\n",
            "           4       0.68      0.93      0.79        14\n",
            "           5       0.75      0.86      0.80        14\n",
            "           6       0.91      0.71      0.80        14\n",
            "           7       0.60      0.86      0.71        14\n",
            "           8       0.71      0.86      0.77        14\n",
            "           9       0.69      0.79      0.73        14\n",
            "          10       0.86      0.86      0.86        14\n",
            "          11       0.79      0.79      0.79        14\n",
            "          12       0.62      0.93      0.74        14\n",
            "          13       0.71      0.71      0.71        14\n",
            "          14       0.71      0.71      0.71        14\n",
            "          15       0.83      0.71      0.77        14\n",
            "          16       0.75      0.64      0.69        14\n",
            "          17       0.54      0.50      0.52        14\n",
            "          18       0.86      0.86      0.86        14\n",
            "          19       0.75      0.64      0.69        14\n",
            "          20       0.64      0.50      0.56        14\n",
            "          21       0.69      0.64      0.67        14\n",
            "          22       1.00      0.79      0.88        14\n",
            "          23       0.71      0.86      0.77        14\n",
            "          24       0.65      0.79      0.71        14\n",
            "          25       0.90      0.64      0.75        14\n",
            "          26       0.83      0.71      0.77        14\n",
            "          27       1.00      0.71      0.83        14\n",
            "          28       0.75      0.64      0.69        14\n",
            "          29       0.72      0.93      0.81        14\n",
            "          30       0.64      0.50      0.56        14\n",
            "          31       0.92      0.79      0.85        14\n",
            "          32       0.86      0.86      0.86        14\n",
            "          33       0.68      0.93      0.79        14\n",
            "          34       1.00      0.57      0.73        14\n",
            "          35       0.75      0.64      0.69        14\n",
            "          36       0.77      0.71      0.74        14\n",
            "          37       0.77      0.71      0.74        14\n",
            "          38       0.74      1.00      0.85        14\n",
            "          39       0.85      0.79      0.81        14\n",
            "          40       0.92      0.86      0.89        14\n",
            "          41       0.92      0.79      0.85        14\n",
            "          42       0.78      0.50      0.61        14\n",
            "\n",
            "    accuracy                           0.76       602\n",
            "   macro avg       0.77      0.76      0.76       602\n",
            "weighted avg       0.77      0.76      0.76       602\n",
            "\n",
            "Saving model and preprocessors...\n",
            "\n",
            "Making predictions for sample inputs:\n",
            "\n",
            "Sample 1:\n",
            "Input Parameters: {'Nitrogen': 90, 'Phosphorus': 42, 'Potassium': 43, 'Temperature': 25, 'Humidity': 82, 'pH': 6.5, 'Soil Type': 'Clayey', 'Period of Month': 'Kharif'}\n",
            "Predicted Crop: Rubber\n",
            "Confidence: 15.71%\n",
            "\n",
            "Top 3 Crop Recommendations:\n",
            "Rubber: 15.71%\n",
            "Ginger: 12.17%\n",
            "Rice: 8.39%\n",
            "\n",
            "Sample 2:\n",
            "Input Parameters: {'Nitrogen': 120, 'Phosphorus': 35, 'Potassium': 30, 'Temperature': 28, 'Humidity': 75, 'pH': 7.0, 'Soil Type': 'Loamy', 'Period of Month': 'Rabi'}\n",
            "Predicted Crop: Ginger\n",
            "Confidence: 36.38%\n",
            "\n",
            "Top 3 Crop Recommendations:\n",
            "Ginger: 36.38%\n",
            "Canola: 9.12%\n",
            "Rice: 8.05%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2732: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2732: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "!pip install optuna\n",
        "!pip install xgboost\n",
        "!pip install imblearn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import optuna\n",
        "import xgboost as xgb\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "class CropPredictor:\n",
        "    def __init__(self):\n",
        "        self.label_encoder_soil = LabelEncoder()\n",
        "        self.label_encoder_period = LabelEncoder()\n",
        "        self.label_encoder_crop = LabelEncoder()\n",
        "        self.scaler = StandardScaler()\n",
        "        self.model = None\n",
        "        self.feature_names = None\n",
        "\n",
        "    def preprocess_data(self, data, is_training=True):\n",
        "        # Create a copy of the data\n",
        "        processed_data = data.copy()\n",
        "\n",
        "        if is_training:\n",
        "            # Fit and transform for training data\n",
        "            processed_data['Soil Type'] = self.label_encoder_soil.fit_transform(processed_data['Soil Type'])\n",
        "            processed_data['Period of Month'] = self.label_encoder_period.fit_transform(processed_data['Period of Month'])\n",
        "        else:\n",
        "            # Transform only for prediction data\n",
        "            processed_data['Soil Type'] = self.label_encoder_soil.transform(processed_data['Soil Type'])\n",
        "            processed_data['Period of Month'] = self.label_encoder_period.transform(processed_data['Period of Month'])\n",
        "\n",
        "        # Feature Engineering\n",
        "        processed_data['NPK_Ratio'] = processed_data['Nitrogen'] / (processed_data['Phosphorus'] + processed_data['Potassium'])\n",
        "        processed_data['Temp_Humidity_Index'] = processed_data['Temperature'] * processed_data['Humidity'] / 100\n",
        "        processed_data['Soil_Moisture_Index'] = np.log1p(processed_data['Temperature'] * processed_data['Humidity'])\n",
        "        processed_data['NPK_TH_Index'] = processed_data['NPK_Ratio'] * processed_data['Temp_Humidity_Index']\n",
        "\n",
        "        return processed_data\n",
        "\n",
        "    def objective(self, trial):\n",
        "        params = {\n",
        "            'n_estimators': trial.suggest_int('n_estimators', 100, 500, step=50),\n",
        "            'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
        "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),\n",
        "            'subsample': trial.suggest_float('subsample', 0.6, 0.9),\n",
        "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 0.9),\n",
        "            'gamma': trial.suggest_float('gamma', 0.01, 0.5, log=True),\n",
        "            'reg_lambda': trial.suggest_float('reg_lambda', 1e-3, 1.0, log=True),\n",
        "            'reg_alpha': trial.suggest_float('reg_alpha', 1e-3, 1.0, log=True)\n",
        "        }\n",
        "\n",
        "        model = xgb.XGBClassifier(**params, objective='multi:softprob', eval_metric='mlogloss', random_state=42)\n",
        "\n",
        "        cv_scores = []\n",
        "        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "        for train_idx, val_idx in cv.split(self.X_train, self.y_train):\n",
        "            X_fold_train, X_fold_val = self.X_train[train_idx], self.X_train[val_idx]\n",
        "            y_fold_train, y_fold_val = self.y_train[train_idx], self.y_train[val_idx]\n",
        "\n",
        "            model.fit(X_fold_train, y_fold_train)\n",
        "            preds = model.predict(X_fold_val)\n",
        "            cv_scores.append(accuracy_score(y_fold_val, preds))\n",
        "\n",
        "        return np.mean(cv_scores)\n",
        "\n",
        "    def train(self, data_path):\n",
        "        print(\"Loading and preprocessing data...\")\n",
        "        data = pd.read_csv(data_path)\n",
        "\n",
        "        # Encode target variable\n",
        "        y = self.label_encoder_crop.fit_transform(data['Crop'])\n",
        "\n",
        "        # Preprocess features\n",
        "        X = self.preprocess_data(data.drop('Crop', axis=1), is_training=True)\n",
        "        self.feature_names = X.columns.tolist()\n",
        "\n",
        "        # Convert to numpy arrays\n",
        "        X = X.values\n",
        "\n",
        "        print(\"Applying SMOTE for class balancing...\")\n",
        "        smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
        "        X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "\n",
        "        self.X_train, X_test, self.y_train, y_test = train_test_split(\n",
        "            X_resampled, y_resampled, test_size=0.2, stratify=y_resampled, random_state=42\n",
        "        )\n",
        "\n",
        "        # Scale features\n",
        "        self.X_train = self.scaler.fit_transform(self.X_train)\n",
        "        X_test = self.scaler.transform(X_test)\n",
        "\n",
        "        print(\"Optimizing hyperparameters...\")\n",
        "        study = optuna.create_study(direction='maximize')\n",
        "        study.optimize(self.objective, n_trials=30, timeout=600)\n",
        "\n",
        "        best_params = study.best_params\n",
        "        print(f\"Best parameters: {best_params}\")\n",
        "        print(f\"Best CV accuracy: {study.best_value:.4f}\")\n",
        "\n",
        "        print(\"Training final model...\")\n",
        "        self.model = xgb.XGBClassifier(\n",
        "            **best_params,\n",
        "            objective='multi:softprob',\n",
        "            eval_metric='mlogloss',\n",
        "            random_state=42\n",
        "        )\n",
        "        self.model.fit(self.X_train, self.y_train)\n",
        "\n",
        "        preds = self.model.predict(X_test)\n",
        "        accuracy = accuracy_score(y_test, preds)\n",
        "        print(f\"\\nFinal Model Accuracy: {accuracy:.4f}\")\n",
        "        print(\"\\nClassification Report:\")\n",
        "        print(classification_report(y_test, preds))\n",
        "\n",
        "        print(\"Saving model and preprocessors...\")\n",
        "        with open('crop_prediction_model.pkl', 'wb') as f:\n",
        "            pickle.dump({\n",
        "                'model': self.model,\n",
        "                'label_encoder_crop': self.label_encoder_crop,\n",
        "                'label_encoder_soil': self.label_encoder_soil,\n",
        "                'label_encoder_period': self.label_encoder_period,\n",
        "                'scaler': self.scaler,\n",
        "                'feature_names': self.feature_names\n",
        "            }, f)\n",
        "\n",
        "        return accuracy\n",
        "\n",
        "    def predict_crop(self, input_data):\n",
        "        # Convert input to DataFrame if it's a dictionary\n",
        "        if isinstance(input_data, dict):\n",
        "            input_data = pd.DataFrame([input_data])\n",
        "\n",
        "        # Preprocess the input data\n",
        "        processed_data = self.preprocess_data(input_data, is_training=False)\n",
        "\n",
        "        # Ensure column order matches training data\n",
        "        processed_data = processed_data[self.feature_names]\n",
        "\n",
        "        # Scale the features\n",
        "        scaled_data = self.scaler.transform(processed_data)\n",
        "\n",
        "        # Make prediction\n",
        "        prediction = self.model.predict(scaled_data)\n",
        "        probabilities = self.model.predict_proba(scaled_data)\n",
        "\n",
        "        predicted_crop = self.label_encoder_crop.inverse_transform(prediction)\n",
        "        return predicted_crop[0], probabilities[0]\n",
        "\n",
        "def main():\n",
        "    predictor = CropPredictor()\n",
        "    accuracy = predictor.train('augmented_smart_crop_data.csv')\n",
        "\n",
        "    # Sample inputs\n",
        "    sample_inputs = [\n",
        "        {\n",
        "            'Nitrogen': 90,\n",
        "            'Phosphorus': 42,\n",
        "            'Potassium': 43,\n",
        "            'Temperature': 25,\n",
        "            'Humidity': 82,\n",
        "            'pH': 6.5,\n",
        "            'Soil Type': 'Clayey',\n",
        "            'Period of Month': 'Kharif'\n",
        "        },\n",
        "        {\n",
        "            'Nitrogen': 120,\n",
        "            'Phosphorus': 35,\n",
        "            'Potassium': 30,\n",
        "            'Temperature': 28,\n",
        "            'Humidity': 75,\n",
        "            'pH': 7.0,\n",
        "            'Soil Type': 'Loamy',\n",
        "            'Period of Month': 'Rabi'\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    print(\"\\nMaking predictions for sample inputs:\")\n",
        "    for i, sample in enumerate(sample_inputs, 1):\n",
        "        predicted_crop, probabilities = predictor.predict_crop(sample)\n",
        "        print(f\"\\nSample {i}:\")\n",
        "        print(f\"Input Parameters: {sample}\")\n",
        "        print(f\"Predicted Crop: {predicted_crop}\")\n",
        "        print(f\"Confidence: {max(probabilities)*100:.2f}%\")\n",
        "\n",
        "        top_3_indices = np.argsort(probabilities)[-3:][::-1]\n",
        "        print(\"\\nTop 3 Crop Recommendations:\")\n",
        "        for idx in top_3_indices:\n",
        "            crop_name = predictor.label_encoder_crop.inverse_transform([idx])[0]\n",
        "            print(f\"{crop_name}: {probabilities[idx]*100:.2f}%\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ]
}